# 强化学习笔记

## 时序差分

 时序差分是介于蒙特卡洛和动态规划之间的方法，它是 model-free 的，不需要马尔可夫决策过程的转移矩阵和奖励函数，时序差分方法可以从不完整的回合中学习。

 $$V(s_t) <-V(s_t)+a(r_{t+1}+)$$
 TD_error = rt+1+rV(st+1)- V(st)

 在蒙特卡洛方法里面，$G_{i,t}$是实际得到的值（可以看成目标），因为它已经把一条轨迹跑完了，可以算出每个状态实际的回报，时序差分方法只执行一步，状态的值就更新。蒙特卡洛方法全部执行完之后，到了终止状态之后，再更新它的值

 之前是只往前走一步，即TD(0)。 我们可以调整步数（step），变成 
n
n步时序差分（
n
n-step TD）。比如 TD(2)，即往前走两步，利用两步得到的回报，使用自举来更新状态的价值。
n趋于∞，td=mc